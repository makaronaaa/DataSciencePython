{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBXZpc8+iOiZyk2rtT6U9N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makaronaaa/DataSciencePython/blob/main/TicTacToe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jeT60_pxw17e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#enviroment\n",
        "class TicTacToe:\n",
        "    def __init__(game): #initializes the game state\n",
        "        game.board = np.zeros((3, 3), dtype=np.int8)  #create a 3x3 board \n",
        "        game.current_player = 1 #set the current_player to 1.\n",
        "\n",
        "      #resets the game state by clearing the board and setting the current_player back to 1.\n",
        "    def reset(game):\n",
        "        game.board = np.zeros((3, 3), dtype=np.int8)\n",
        "        game.current_player = 1\n",
        "\n",
        "    def get_state(game):  #returns a copy of the current board state.\n",
        "        return np.copy(game.board)\n",
        "\n",
        "    def make_move(game, row, col): #allows a player to make a move on the board.\n",
        "        if game.board[row][col] == 0: #if position is empty (marked as 0), \n",
        "            game.board[row][col] = game.current_player   #updates the board with the current player's marker (1 or -1) \n",
        "            game.current_player = -game.current_player #and changes the current_player to the opposite player. \n",
        "\n",
        "            return True #if the move is valid\n",
        "        return False #otherwise\n",
        "\n",
        "    def is_game_over(game):  #whether the game is over or not.\n",
        "    #all possible winning combinations\n",
        "        for i in range(3):\n",
        "            if game.board[i][0] == game.board[i][1] == game.board[i][2] != 0:\n",
        "                return game.board[i][0]\n",
        "\n",
        "        for i in range(3):\n",
        "            if game.board[0][i] == game.board[1][i] == game.board[2][i] != 0:\n",
        "                return game.board[0][i]\n",
        "\n",
        "        if game.board[0][0] == game.board[1][1] == game.board[2][2] != 0:\n",
        "            return game.board[0][0]\n",
        "        if game.board[0][2] == game.board[1][1] == game.board[2][0] != 0:\n",
        "            return game.board[0][2]\n",
        "\n",
        "        if np.count_nonzero(game.board) == 9:\n",
        "            return 0  #0 if the game is a draw\n",
        "        return None # game isnt over yet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#     Reinforcement Learning Agent\n",
        "class RLAgent:\n",
        "    def __init__(game):\n",
        "        game.model = game.build_model()  #create a neural network model using the build_model function\n",
        "        game.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    def build_model(game):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(9,)),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(9, activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=game.optimizer)\n",
        "        return model\n",
        "\n",
        "    def get_action(game, state):\n",
        "        q_values = game.model.predict(np.expand_dims(state.flatten(), axis=0))\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def train(game, states, target_values):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = game.model(states, training=True)\n",
        "            loss = tf.reduce_mean(tf.square(target_values - predictions)) #MSE= (x-pred(x))^2/n\n",
        "        gradients = tape.gradient(loss, game.model.trainable_variables)\n",
        "        game.optimizer.apply_gradients(zip(gradients, game.model.trainable_variables)) #optimizer applies the gradients to update the model weights.\n",
        "\n",
        "    def train_with_human_feedback(game, states, human_labels):\n",
        "        target_values = np.zeros((len(states), 9))\n",
        "        for i, label in enumerate(human_labels):\n",
        "            target_values[i][label] = 1\n",
        "        game.train(states, target_values)\n",
        "\n",
        "def get_human_move():\n",
        "    while True:\n",
        "        try:\n",
        "            move = int(input(\"Enter your move (   ): \"))\n",
        "            row = (move - 1) // 3  #calculate the row index\n",
        "            col = (move - 1) % 3 #calculate the column index\n",
        "            return row, col\n",
        "        except ValueError:\n",
        "            print(\"Invalid input. Try again.\")"
      ],
      "metadata": {
        "id": "zO6Qbx62VEL6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLAgent:\n",
        "    def __init__(game, optimizer):\n",
        "        game.model = game.build_model(optimizer)\n",
        "        game.optimizer = optimizer\n",
        "\n",
        "    def build_model(game, optimizer):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(64, activation='relu', input_shape=(9,)),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dense(9, activation='softmax')\n",
        "        ])\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
        "        return model\n",
        "\n",
        "    def get_action(game, state):\n",
        "        q_values = game.model.predict(np.expand_dims(state.flatten(), axis=0))\n",
        "        return np.argmax(q_values)"
      ],
      "metadata": {
        "id": "V4wLZwruVbDl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "env = TicTacToe()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "agent = RLAgent(optimizer)\n",
        "\n",
        "num_episodes = 1\n",
        "epsilon = 0.2\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    env.reset()\n",
        "    done = False\n",
        "    states = []\n",
        "    human_labels = []\n",
        "\n",
        "    while not done:\n",
        "        # agent's turn\n",
        "        state = env.get_state()\n",
        "        action = agent.get_action(state)\n",
        "        states.append(state.flatten())\n",
        "\n",
        "        #human's turn\n",
        "        human_row, human_col = get_human_move()\n",
        "        human_labels.append(human_row * 3 + human_col)\n",
        "        env.make_move(human_row, human_col)\n",
        "        print(\"Human's move:\")\n",
        "        print(env.board)\n",
        "\n",
        "        done = env.is_game_over()\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        #agent makes a move\n",
        "        if np.random.random() < epsilon:\n",
        "            rl_row, rl_col = np.random.randint(0, 3), np.random.randint(0, 3)\n",
        "        else:\n",
        "            q_values = agent.model.predict(np.expand_dims(state.flatten(), axis=0))\n",
        "            valid_moves = np.where(env.board == 0)\n",
        "            valid_moves = np.column_stack(valid_moves)\n",
        "            valid_q_values = [q_values[0][move[0] * 3 + move[1]] for move in valid_moves]\n",
        "            best_move_idx = np.argmax(valid_q_values)\n",
        "            rl_row, rl_col = valid_moves[best_move_idx]\n",
        "\n",
        "        env.make_move(rl_row, rl_col)\n",
        "        print(\"RL Agent's move:\")\n",
        "        print(env.board)\n",
        "\n",
        "        done = env.is_game_over()\n",
        "\n",
        "    #update RL Agent with human feedback\n",
        "    agent.train_with_human_feedback(np.array(states), np.array(human_labels))\n",
        "\n",
        "    if done == 1:\n",
        "        print(\"agent wins!\")\n",
        "    elif done == -1:\n",
        "        print(\"real human wins!\")\n",
        "    else:\n",
        "        print(\"draw!\")\n",
        "\n",
        "    if episode % 100 == 0:\n",
        "        print(f\"Episode: {episode}\")\n",
        "\n",
        "print(\"Training Finito!\")\n"
      ],
      "metadata": {
        "id": "XMf3-ja8YuhW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}